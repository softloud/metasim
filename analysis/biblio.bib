@Article{pinheiro_d-dimer_2012,
  langid = {english},
  title = {D-Dimer in Preeclampsia: Systematic Review and Meta-Analysis},
  volume = {414},
  issn = {1873-3492},
  doi = {10.1016/j.cca.2012.08.003},
  shorttitle = {D-Dimer in Preeclampsia},
  abstract = {Preeclampsia is a multifactorial disease characterized by high blood pressure and proteinuria after the 20th week of pregnancy. Preeclampsia is associated with microvasculature fibrin deposition and maternal organ dysfunction. D-dimer (D-Di) has been used as a marker of production/degradation of fibrin in vivo. D-Di has emerged as a useful diagnostic tool for thrombotic conditions because its plasma concentration has a high negative predictive value for venous thromboembolism. The aim of this study was to evaluate publications that assessed plasma D-Di in preeclampsia and normotensive pregnant subjects to define its diagnostic value. A total of 194 publications were identified. Following the exclusion process, seven studies were in accordance with the pre-defined eligibility criteria. This systematic review was performed with methodologic accuracy, including a careful definition of preeclampsia and a high sensitivity literature search strategy. Quality of the included studies was assessed in accordance with widely accepted literature recommendations. Our meta-analysis indicates that increased plasma D-Di is associated with preeclampsia in the third trimester of gestation vs normotensive pregnant subjects. These preliminary findings in this select group of patients clearly highlight the need for additional comprehensive studies throughout pregnancy, including the establishment of an appropriate cut-off, in order to fully elucidate the diagnostic/prognostic role of D-Di in preeclampsia.},
  journaltitle = {Clinica Chimica Acta; International Journal of Clinical Chemistry},
  shortjournal = {Clin. Chim. Acta},
  date = {2012-12-24},
  pages = {166-170},
  keywords = {Humans,Meta-Analysis as Topic,Enzyme-Linked Immunosorbent Assay,Female,Fibrin Fibrinogen Degradation Products,Pre-Eclampsia,Pregnancy},
  author = {Melina de Barros Pinheiro and Daniela Rezende Garcia Junqueira and Fernanda Fonseca Coelho and Let{\a'\i}cia G. Freitas and Maria G. Carvalho and Karina Braga Gomes and Luci Maria Santana Dusse},
  eprinttype = {pmid},
  eprint = {22922438},
}
@Article{bland_estimating_2014,
  langid = {english},
  title = {Estimating {{Mean}} and {{Standard Deviation}} from the {{Sample Size}}, {{Three Quartiles}}, {{Minimum}}, and {{Maximum}}},
  volume = {4},
  issn = {1929-6029},
  url = {http://lifescienceglobal.com/pms/index.php/ijsmr/article/view/2688},
  abstract = {Background: We sometimes want to include in a meta-analysis data from studies where results are presented as medians and ranges or interquartile ranges rather than as means and standard deviations. In this paper I extend a method of Hozo et al. to estimate mean and standard deviation from median, minimum, and maximum to the case where quartiles are also available.Methods: Inequalities are developed for each observation using upper and lower limits derived from the minimum, the three quartiles, and the maximum. These are summed to give bounds for the sum and hence the mean of the observations, the average of these bounds in the estimate. A similar estimate is found for the sum of the observations squared and hence for the variance and standard deviation.Results: For data from a Normal distribution, the extended method using quartiles gives good estimates of sample means but sample standard deviations are overestimated. For data from a Lognormal distribution, both sample mean and standard deviation are overestimated. Overestimation is worse for larger samples and for highly skewed parent distributions. The extended estimates using quartiles are always superior in both bias and precision to those without.Conclusions: The estimates have the advantage of being extremely simple to carry out. I argue that as, in practice, such methods will be applied to small samples, the overestimation may not be a serious problem.},
  number = {1},
  journaltitle = {International Journal of Statistics in Medical Research},
  urldate = {2019-04-09},
  date = {2014-01-27},
  pages = {57-64-64},
  keywords = {maximum,mean,minimum,Quartile,standard deviation,systematic review.},
  author = {Martin Bland},
  file = {/home/charles/Zotero/storage/Z6BPKFY9/Bland - 2014 - Estimating Mean and Standard Deviation from the Sa.pdf;/home/charles/Zotero/storage/6XSYVQI8/2688.html},
}
@Article{parker_opinionated_2017,
  langid = {english},
  title = {Opinionated Analysis Development},
  doi = {10.7287/peerj.preprints.3210v1},
  abstract = {Traditionally, statistical training has focused primarily on mathematical derivations and proofs of statistical tests. The process of developing the technical artifact—that is, the paper, dashboard, or other deliverable—is much less frequently taught, presumably because of an aversion to cookbookery or prescribing specific software choices. In this paper I argue that it’s critical to teach analysts how to go about developing an analysis in order to maximize the probability that their analysis is reproducible, accurate, and collaborative. A critical component of this is adopting a blameless postmortem culture. By encouraging the use of and fluency in tooling that implements these opinions, as well as a blameless way of correcting course as analysts encounter errors, we as a community can foster the growth of processes that fail the practitioners as infrequently as possible.},
  journaltitle = {preprint},
  date = {2017},
  keywords = {dumpsterfire,measured.},
  author = {Hilary Parker},
  file = {/home/charles/Zotero/storage/W2AAN92U/Parker - Opinionated analysis development.pdf},
}
@Article{marwick_packaging_2018,
  title = {Packaging {{Data Analytical Work Reproducibly Using R}} (and {{Friends}})},
  volume = {72},
  issn = {0003-1305},
  url = {https://doi.org/10.1080/00031305.2017.1375986},
  doi = {10.1080/00031305.2017.1375986},
  abstract = {Computers are a central tool in the research process, enabling complex and large-scale data analysis. As computer-based research has increased in complexity, so have the challenges of ensuring that this research is reproducible. To address this challenge, we review the concept of the research compendium as a solution for providing a standard and easily recognizable way for organizing the digital materials of a research project to enable other researchers to inspect, reproduce, and extend the research. We investigate how the structure and tooling of software packages of the R programming language are being used to produce research compendia in a variety of disciplines. We also describe how software engineering tools and services are being used by researchers to streamline working with research compendia. Using real-world examples, we show how researchers can improve the reproducibility of their work using research compendia based on R packages and related tools.},
  number = {1},
  journaltitle = {The American Statistician},
  urldate = {2018-11-24},
  date = {2018-01-02},
  pages = {80-88},
  keywords = {measured.,reproducibility,Computational science,Data science,Open source software,packaged data analysis,Reproducible research},
  author = {Ben Marwick and Carl Boettiger and Lincoln Mullen},
  file = {/home/charles/Zotero/storage/MEBP88H9/Marwick et al. - 2018 - Packaging Data Analytical Work Reproducibly Using .pdf;/home/charles/Zotero/storage/K524VAIS/00031305.2017.html},
}

@Article{wan_estimating_2014,
  title = {Estimating the Sample Mean and Standard Deviation from the Sample Size, Median, Range and/or Interquartile Range},
  volume = {14},
  issn = {1471-2288},
  url = {https://doi.org/10.1186/1471-2288-14-135},
  doi = {10.1186/1471-2288-14-135},
  abstract = {In systematic reviews and meta-analysis, researchers often pool the results of the sample mean and standard deviation from a set of similar clinical trials. A number of the trials, however, reported the study using the median, the minimum and maximum values, and/or the first and third quartiles. Hence, in order to combine results, one may have to estimate the sample mean and standard deviation for such trials.},
  number = {1},
  journaltitle = {BMC Medical Research Methodology},
  shortjournal = {BMC Medical Research Methodology},
  urldate = {2019-02-10},
  date = {2014-12-19},
  pages = {135},
  author = {Xiang Wan and Wenqian Wang and Jiming Liu and Tiejun Tong},
  file = {/home/charles/Zotero/storage/K3JRCUMD/Wan et al. - 2014 - Estimating the sample mean and standard deviation .pdf;/home/charles/Zotero/storage/T7IXANSG/Wan et al. - 2014 - Estimating the sample mean and standard deviation .pdf;/home/charles/Zotero/storage/QB9727I7/1471-2288-14-135.html;/home/charles/Zotero/storage/ZYZWSTZJ/1471-2288-14-135.html},
}

@Article{hozo_estimating_2005,
  title = {Estimating the Mean and Variance from the Median, Range, and the Size of a Sample},
  volume = {5},
  issn = {1471-2288},
  url = {https://doi.org/10.1186/1471-2288-5-13},
  doi = {10.1186/1471-2288-5-13},
  abstract = {Usually the researchers performing meta-analysis of continuous outcomes from clinical trials need their mean value and the variance (or standard deviation) in order to pool data. However, sometimes the published reports of clinical trials only report the median, range and the size of the trial.},
  number = {1},
  journaltitle = {BMC Medical Research Methodology},
  shortjournal = {BMC Medical Research Methodology},
  urldate = {2019-04-09},
  date = {2005-04-20},
  pages = {13},
  author = {Stela Pudar Hozo and Benjamin Djulbegovic and Iztok Hozo},
  file = {/home/charles/Zotero/storage/9YXNJNB3/Hozo et al. - 2005 - Estimating the mean and variance from the median, .pdf;/home/charles/Zotero/storage/25FNU33N/1471-2288-5-13.html},
}
@InCollection{pardalos_sampling_2016,
  langid = {english},
  location = {{Cham}},
  title = {On {{Sampling Methods}} for {{Costly Multi}}-{{Objective Black}}-{{Box Optimization}}},
  volume = {107},
  isbn = {978-3-319-29973-0 978-3-319-29975-4},
  url = {http://link.springer.com/10.1007/978-3-319-29975-4_15},
  abstract = {We investigate the impact of diﬀerent sampling techniques on the performance of multiobjective optimization methods applied to costly black-box optimization problems. Such problems are often solved using an algorithm in which a surrogate model approximates the true objective function and provides predicted objective values at a lower cost. As the surrogate model is based on evaluations of a small number of points, the quality of the initial sample can have a great eﬀect on the overall eﬀectiveness of the optimization. In this study, we demonstrate how various sampling techniques aﬀect the results of applying diﬀerent optimization algorithms to a set of benchmark problems. Additionally, some recommendations on usage of sampling methods are provided.},
  booktitle = {Advances in {{Stochastic}} and {{Deterministic Global Optimization}}},
  publisher = {{Springer International Publishing}},
  urldate = {2019-03-18},
  date = {2016},
  pages = {273-296},
  author = {Ingrida Steponavic and Mojdeh Shirazi-Manesh and Rob J. Hyndman and Kate Smith-Miles and Laura Villanova},
  editor = {Panos M. Pardalos and Anatoly Zhigljavsky and Julius Zilinskas},
  file = {/home/charles/Zotero/storage/3KGBI7WT/Steponavičė et al. - 2016 - On Sampling Methods for Costly Multi-Objective Bla.pdf},
  doi = {10.1007/978-3-319-29975-4_15},
}
@Article{wolpert_no_1997,
  langid = {english},
  title = {No Free Lunch Theorems for Optimization},
  volume = {1},
  issn = {1089778X},
  url = {http://ieeexplore.ieee.org/document/585893/},
  doi = {10.1109/4235.585893},
  abstract = {A framework is developed to explore the connection between e ective optimization algorithms and the problems they are solving. A number of \textbackslash{}no free lunch{"} (NFL) theorems are presented that establish that for any algorithm, any elevated performance over one class of problems is exactly paid for in performance over another class. These theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem. Applications of the NFL theorems to information theoretic aspects of optimization and benchmark measures of performance are also presented. Other issues addressed are time-varying optimization problems and a priori \textbackslash{}head-to-head{"} minimax distinctions between optimization algorithms, distinctions that can obtain despite the NFL theorems' enforcing of a type of uniformity over all algorithms.},
  number = {1},
  journaltitle = {IEEE Transactions on Evolutionary Computation},
  shortjournal = {IEEE Trans. Evol. Computat.},
  urldate = {2019-05-04},
  date = {1997-04},
  pages = {67-82},
  author = {D.H. Wolpert and W.G. Macready},
  file = {/home/charles/Zotero/storage/V77DFL6B/Wolpert and Macready - 1997 - No free lunch theorems for optimization.pdf},
}
